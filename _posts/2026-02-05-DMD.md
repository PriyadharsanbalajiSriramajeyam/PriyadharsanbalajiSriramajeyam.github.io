---
layout: post
title: Flow Models
description: DMD
date: 2026-02-05 16:40:16
tags: GenerativeAI
image: img/postbanners/2024-08-07-convert-datetime2-bigint.png
---


# Distribution Matching Distillation (DMD) for Faster Diffusion Sampling

<div align="center">
    <a href="https://drive.google.com/file/d/1cwzSPKHALoCr25nBZ0kTjuYWrtXhSK8L/view?usp=drive_link">
        <img src="https://drive.google.com/thumbnail?id=1cwzSPKHALoCr25nBZ0kTjuYWrtXhSK8L&sz=w1000" 
             alt="DMD Process" 
             style="max-width: 100%; height: auto;">
    </a>
</div>


<div align="center">
    <a href="https://drive.google.com/file/d/15uX-GbJcQDKGVg0LAiey2RKOjd-z7Y7g/view?usp=drive_link">
        <img src="https://drive.google.com/thumbnail?id=15uX-GbJcQDKGVg0LAiey2RKOjd-z7Y7g&sz=w1000" 
             alt="ECCV Workshop Picture" 
             style="max-width: 100%; height: auto;">
    </a>
</div>


Table of Contents

- [1. The Core Idea](#the-core-idea)
- [2. The Math](#the-math)
- [3. Math and Code Side by Side](#math-and-code-side-by-side)
- [4. Training Algorithm](#training-algorithm)

---




## The Core Idea

Consider a diffusion model trained on ImageNet that requires 50 DDIM steps to generate high-quality samples. Even with improved samplers such as EDM, you are still looking at roughly 10 steps for acceptable outputs. The question is straightforward: can we get this down to a single step without losing quality? Distribution Matching Distillation (DMD) is one approach that does exactly that.

DMD is built on two ideas that, on their own, are well-studied. The first is *distribution matching* — aligning two probability distributions induced by different generative processes so that they are as close as possible. The second is *knowledge distillation* — training a smaller or cheaper model (the student) to reproduce the output behavior of a stronger model (the teacher), transferring performance while cutting inference cost.

The standard tool for comparing distributions here is the Kullback–Leibler (KL) divergence. In classical distillation you typically match student outputs to teacher outputs at the sample level. DMD works at the *distribution* level instead, which turns out to be the key design choice that makes single-step generation feasible.

The method involves three models, each with a distinct role.

### The Teacher — A Frozen Base Diffusion Model

This is your pretrained diffusion model. It already captures the data distribution well, but sampling from it is expensive because of the multi-step denoising trajectory. In DMD it is frozen throughout training. It defines the reference distribution that everything else is trying to match.

### The Student — A One-Step Generator

The student is a single-network generator $G_\theta$ that maps noise directly to images in one forward pass. It cannot replicate the teacher's iterative denoising, so instead of matching the teacher step-by-step, it is trained with a distribution-level objective: make $p_{\text{fake}}$ close to $p_{\text{real}}$.

### The Critic — A Trainable Fake Diffusion Model

Pure distribution matching is unstable in practice. To fix this, DMD introduces an auxiliary diffusion-style model $\mu_\phi^{\text{fake}}$ that is trained on the student's *current* outputs. It continuously adapts as the student changes, and its job is to provide reliable gradient signals for the student. Without it, the distribution-matching signal becomes noisy enough that training collapses.

---

## The Math

### Notation

Throughout this section we use the following parameterizations:

- Base diffusion model (teacher, frozen): $\mu_{\text{base}}(x_t, t)$
- Fake diffusion model (critic, trainable): $\mu_\phi^{\text{fake}}(x_t, t)$
- One-step generator (student, trainable): $G_\theta(z)$

### Distributions and the Matching Objective

We have two distributions over images. The real data distribution is simply $x \sim p_{\text{real}}(x)$. The fake distribution is defined by sampling Gaussian noise and pushing it through the generator:

$$
z \sim \mathcal{N}(0, I), \qquad x = G_\theta(z), \qquad x \sim p_{\text{fake}}(x).
$$

The objective is *not* to match individual samples one-to-one. It is to make the *overall* distribution $p_{\text{fake}}$ as close as possible to $p_{\text{real}}$.

### Why KL Divergence

The KL divergence between these two distributions is

$$
D_{\text{KL}}\bigl(p_{\text{fake}} \,\|\, p_{\text{real}}\bigr)
= \mathbb{E}_{x \sim p_{\text{fake}}}
\Bigl[
\log p_{\text{fake}}(x) - \log p_{\text{real}}(x)
\Bigr].
$$

The intuition is simple. If the generator places mass in regions where $p_{\text{real}}(x)$ is low, then $\log p_{\text{real}}(x)$ is very negative and the divergence is large. If $p_{\text{fake}} \approx p_{\text{real}}$, the divergence is small. Training is done via gradient descent, so the quantity we actually need is $\nabla_\theta D_{\text{KL}}$.

### Gradient Derivation

Starting from the KL expression, we reparameterize samples from $p_{\text{fake}}$ using the generator $x = G_\theta(z)$:

$$
D_{\text{KL}}\bigl(p_{\text{fake}} \,\|\, p_{\text{real}}\bigr)
= \mathbb{E}_{z \sim \mathcal{N}(0,I)}
\Bigl[
\log p_{\text{fake}}\bigl(G_\theta(z)\bigr)
-
\log p_{\text{real}}\bigl(G_\theta(z)\bigr)
\Bigr].
$$

Differentiating with respect to $\theta$:

$$
\nabla_{\theta} D_{\text{KL}}\bigl(p_{\text{fake}} \,\|\, p_{\text{real}}\bigr)
=
\mathbb{E}_{z}
\Bigl[
\nabla_{\theta}\log p_{\text{fake}}(x)
-
\nabla_{\theta}\log p_{\text{real}}(x)
\Bigr].
$$

Now apply the chain rule. For any density $p(x)$ where $x = G_\theta(z)$:

$$
\nabla_{\theta}\log p(x)
=
\nabla_{x}\log p(x)\;
\frac{\partial x}{\partial \theta}.
$$

Substituting this in for both terms gives

$$
\nabla_{\theta} D_{\text{KL}}\bigl(p_{\text{fake}} \,\|\, p_{\text{real}}\bigr)
=
\mathbb{E}_{z}
\Bigl[
\bigl(
\nabla_{x}\log p_{\text{fake}}(x)
-
\nabla_{x}\log p_{\text{real}}(x)
\bigr)
\frac{\partial G_{\theta}(z)}{\partial \theta}
\Bigr].
$$

### Score Functions

The terms $\nabla_x \log p(x)$ have a name: they are the *score functions* of the respective distributions.

$$
s_{\text{real}}(x) = \nabla_{x}\log p_{\text{real}}(x),
\qquad
s_{\text{fake}}(x) = \nabla_{x}\log p_{\text{fake}}(x).
$$

The score at a point $x$ points in the direction of steepest increase of the log-density. Perturbing an image slightly in the direction $s(x)$ makes it more likely under $p$. With this notation the gradient becomes

$$
\nabla_{\theta} D_{\text{KL}}\bigl(p_{\text{fake}} \,\|\, p_{\text{real}}\bigr)
=
\mathbb{E}_{z}
\Bigl[
-\bigl(
s_{\text{real}}(x)
-
s_{\text{fake}}(x)
\bigr)
\frac{\partial G_{\theta}(z)}{\partial \theta}
\Bigr].
$$

The problem reduces to estimating these two score functions. This is where the diffusion models come in.

### Forward Process and Score Estimation via Score-SDE

Apply the standard forward noising process to any image $x$:

$$
x_t = \alpha_t\, x + \sigma_t\, \epsilon,
\qquad
\epsilon \sim \mathcal{N}(0, I).
$$

Score-SDE theory tells us that at noise level $t$, the score of the Gaussian-perturbed distribution can be written in terms of a mean-prediction network. For the real distribution (teacher, frozen):

$$
s_{\text{real}}(x_t,t)
=
-\frac{x_t - \alpha_t\,\mu_{\text{base}}(x_t,t)}{\sigma_t^2}.
$$

For the fake distribution (critic, trainable):

$$
s_{\text{fake}}(x_t,t)
=
-\frac{x_t - \alpha_t\,\mu^{\text{fake}}_{\phi}(x_t,t)}{\sigma_t^2}.
$$

Subtracting the two eliminates the $x_t$ terms cleanly:

$$
s_{\text{fake}}(x_t,t) - s_{\text{real}}(x_t,t)
=
\frac{\alpha_t}{\sigma_t^2}
\Bigl(
\mu^{\text{fake}}_{\phi}(x_t,t)
-
\mu_{\text{base}}(x_t,t)
\Bigr).
$$

This is a concrete, computable quantity — the difference in mean predictions between the two diffusion models at noise level $t$.

### The Distribution-Matching Gradient

Putting everything together, the approximate distribution-matching gradient is

$$
\nabla_{\theta} D_{\text{KL}}
\approx
\mathbb{E}_{z,\,t,\,x_t}
\Bigl[
w_t\,\alpha_t\,
\bigl(
s_{\text{fake}}(x_t,t)
-
s_{\text{real}}(x_t,t)
\bigr)
\frac{\partial G_{\theta}(z)}{\partial \theta}
\Bigr].
$$

The weighting $w_t$ is chosen as

$$
w_t
=
\sigma_t^2\,\alpha_t\,C_S\,
\|\mu_{\text{base}}(x_t,t) - x\|_1,
$$

which down-weights timesteps where the teacher's prediction is already close to the clean image.

### Preventing Mode Dropping — The Regression Anchor

Distribution matching alone can suffer from mode dropping: the generator finds a subset of modes that minimizes the divergence and ignores the rest. To prevent this, DMD adds a pointwise regression loss anchored to the teacher.

First, a paired dataset is constructed offline by running the teacher's ODE sampler on a set of noise vectors:

$$
\mathcal{D}
=
\{(z_i,\, y_i)\},
\qquad
y_i
=
\text{ODE-solve}(\mu_{\text{base}},\, z_i).
$$

The generator is then trained with an additional regression objective on these pairs:

$$
\mathcal{L}_{\text{reg}}
=
\mathbb{E}_{(z,\,y) \sim \mathcal{D}}
\bigl[
\ell\bigl(G_{\theta}(z),\, y\bigr)
\bigr],
$$

where $\ell$ is LPIPS. This loss preserves the global structure of the teacher's output and significantly reduces mode collapse.

---


<div align="center">
    <a href="https://drive.google.com/file/d/1tQekA_co_mqh85-gGee4035ujVa-shlA/view?usp=drive_link">
        <img src="https://drive.google.com/thumbnail?id=1tQekA_co_mqh85-gGee4035ujVa-shlA&sz=w1000" 
             alt="ECCV Workshop Picture" 
             style="max-width: 100%; height: auto;">
    </a>
</div>


<div align="center">
    <a href="https://drive.google.com/file/d/1jvFbc9-BSe5w8gtns4AaAf0ZhtTPz13E/view?usp=drive_link">
        <img src="https://drive.google.com/thumbnail?id=1jvFbc9-BSe5w8gtns4AaAf0ZhtTPz13E&sz=w1000" 
             alt="ECCV Workshop Picture" 
             style="max-width: 100%; height: auto;">
    </a>
</div>




## Math and Code Side by Side

The table below maps each mathematical component of DMD to its PyTorch implementation. The code is simplified and meant to make the correspondence explicit rather than to be a drop-in training loop.

| **Math** | **PyTorch** |
|---|---|
| **Generator-induced distribution** $z \sim \mathcal{N}(0,I),\; x = G_\theta(z)$ | `z = torch.randn(B, C, device=device)` <br> `x_fake = generator(z)` |
| **Forward diffusion (EDM-style)** $x_t = x + \sigma_t \epsilon,\; \epsilon \sim \mathcal{N}(0,I),\; \alpha_t = 1$ | `noise = torch.randn_like(x)` <br> `x_t = x + sigma.unsqueeze(1) * noise` |
| **EDM-preconditioned denoised estimate** $\hat{x}_\psi(x_t,\sigma) = c_{\text{skip}}(\sigma)\,x_t + c_{\text{out}}(\sigma)\,f_\psi(c_{\text{in}}(\sigma)\,x_t,\,\sigma)$ | `eps = model(c_in * x_t, sigma)` <br> `x_pred = c_skip * x_t + c_out * eps` |
| **Score function from denoiser** $s_\psi(x_t,\sigma) \approx \frac{\hat{x}_\psi(x_t,\sigma) - x_t}{\sigma^2}$ | `score = (x_pred - x_t) / (sigma**2 + 1e-8)` |
| **KL gradient as score difference** $\nabla_\theta D_{\text{KL}} \propto \mathbb{E}\bigl[(s_{\text{fake}} - s_{\text{real}})\;\partial_\theta G_\theta\bigr]$ | `s_real = compute_score(teacher, x_t, sigma)` <br> `s_fake = compute_score(fake_model, x_t, sigma)` <br> `grad = s_fake - s_real` |
| **Noise-dependent weighting** $w(\sigma, x) \propto \frac{\sigma^2 + 1}{\|\hat{x}_{\text{real}}(x_t,\sigma) - x\|_1 + \varepsilon}$ | `weight = (sigma**2 + 1) / (` <br> `    torch.abs(x_pred - x_fake).mean(1, keepdim=True) + 1e-5)` <br> `weight = weight.clamp(0.1, 10.0)` |
| **Distribution matching loss (stop-gradient proxy)** $\mathcal{L}_{\text{DM}} = \tfrac{1}{2}\|x - \text{stopgrad}(x - w\,\Delta s)\|_2^2$ where $\Delta s = s_{\text{fake}} - s_{\text{real}}$ | `loss_dm = 0.5 * (` <br> `    x_fake - (x_fake - weight * grad).detach()` <br> `).pow(2).mean()` |
| **Paired teacher anchor** $y = \text{ODE-solve}(\mu_{\text{base}},\, z)$ | `y = sample_edm(teacher, z)` <br> `paired_data.append((z, y))` |
| **Regression (anchor) loss** $\mathcal{L}_{\text{reg}} = \mathbb{E}\bigl[\|G_\theta(z) - y\|_2^2\bigr]$ | `loss_reg = ((x_ref - y_ref)**2).mean()` |
| **Total generator objective** $\mathcal{L}_{\text{gen}} = \mathcal{L}_{\text{DM}} + \lambda_{\text{reg}}\,\mathcal{L}_{\text{reg}}$ | `loss_gen = loss_dm + LAMBDA_REG * loss_reg` <br> `loss_gen.backward()` <br> `opt_gen.step()` |

---

## Training Algorithm

Each training iteration has two updates: one for the generator and one for the fake diffusion model.

### Setup

**Given:** frozen teacher $\mu_{\text{base}}$, one-step generator $G_\theta$, trainable fake diffusion model $\mu_\phi^{\text{fake}}$, noise schedule $\sigma(t)$ for $t \in [0, 1000]$, regression weight $\lambda_{\text{reg}}$.

**Before training begins:** freeze the teacher, initialize $G_\theta$ and $\mu_\phi^{\text{fake}}$ from the teacher weights, create an EMA copy $\bar{G}_\theta$ of the generator, and precompute the paired dataset $\mathcal{D} = \{(z_i, y_i)\}$ where $y_i = \text{TeacherSample}(\mu_{\text{base}}, z_i)$.

### Step A — Generator Update

1. Sample noise and generate a fake image:
$$
z \sim \mathcal{N}(0, I), \qquad x_{\text{fake}} = G_\theta(z).
$$

2. Sample a noise level $t$ and apply the forward diffusion process:
$$
x_t = x_{\text{fake}} + \sigma(t)\,\epsilon, \qquad \epsilon \sim \mathcal{N}(0, I).
$$

3. Compute the two score estimates. The teacher score is frozen:
$$
s_{\text{real}} = \nabla_{x_t} \log p_{\text{real}}(x_t).
$$
The fake score uses the critic:
$$
s_{\text{fake}} = \nabla_{x_t} \log p_{\text{fake}}(x_t).
$$

4. Form the weighted score difference:
$$
\Delta s = w(t)\,\bigl(s_{\text{fake}} - s_{\text{real}}\bigr).
$$

5. Compute the distribution-matching loss using a stop-gradient proxy. The stop-gradient is what makes this differentiable with respect to $\theta$ without backpropagating through the score networks:
$$
\mathcal{L}_{\text{DM}}
=
\frac{1}{2}
\left\|
x_{\text{fake}}
-
\text{stopgrad}
\bigl(x_{\text{fake}} - \Delta s\bigr)
\right\|_2^2.
$$

6. Sample a paired target $(z, y) \sim \mathcal{D}$ and compute the regression anchor loss:
$$
\mathcal{L}_{\text{reg}}
=
\|G_\theta(z) - y\|_2^2.
$$

7. Update the generator:
$$
\mathcal{L}_{\text{gen}}
=
\mathcal{L}_{\text{DM}}
+
\lambda_{\text{reg}}\,\mathcal{L}_{\text{reg}}.
$$

### Step B — Fake Diffusion Model Update

1. Generate a new batch of fake images:
$$
z \sim \mathcal{N}(0, I), \qquad x_{\text{fake}} = G_\theta(z).
$$

2. Apply the forward diffusion process:
$$
x_t = x_{\text{fake}} + \sigma(t)\,\epsilon.
$$

3. Run the fake diffusion model to get a denoised prediction:
$$
\hat{x}_{\text{fake}} = \mu^{\text{fake}}_\phi(x_t, t).
$$

4. Compute the denoising loss:
$$
\mathcal{L}_{\text{fake}}
=
\mathbb{E}\!\left[
w(t)\,\|\hat{x}_{\text{fake}} - x_{\text{fake}}\|_2^2
\right].
$$

5. Update the fake diffusion parameters:
$$
\phi \leftarrow \phi - \eta\,\nabla_\phi \mathcal{L}_{\text{fake}}.
$$

### Output

At the end of training, the EMA generator $\bar{G}_\theta$ is the final model. It produces high-quality samples in a single forward pass.

---

To summarize the role of each component: the generator learns *where* probability mass should move, guided by the score differences. The fake diffusion model learns *what the generator's current distribution looks like*, so that the score signal stays accurate as training progresses. The teacher anchors both to the true data manifold through the regression pairs.


