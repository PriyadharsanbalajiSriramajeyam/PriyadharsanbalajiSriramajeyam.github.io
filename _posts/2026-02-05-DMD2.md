---
layout: post
title: Distribution Matching Distillation v2 (DMD2)
description: A structured walkthrough of DMD2 — removing the regression anchor, stabilizing pure distribution matching with TTUR, integrating a GAN objective, and extending to multi-step generators via backward simulation.
date: 2025-08-12 10:30:00
tags: ComputerVision GenerativeAI
image: img/postbanners/dmd2-banner.png
---

# Distribution Matching Distillation 2 (DMD2)

<div align="center">
  <a href="https://drive.google.com/file/d/1fcSnG7adJgP0f55vDBaqX368O2JkaF-f/view?usp=drive_link">
    <img src="https://drive.google.com/thumbnail?id=1fcSnG7adJgP0f55vDBaqX368O2JkaF-f&sz=w1000"
         alt="DMD Process"
         style="max-width: 100%; height: auto;">
  </a>
</div>
<div align="center">
  <a href="https://drive.google.com/file/d/1wjmoXu8vDhZXLRkSc_gd4nLa1-BJqCqa/view?usp=drive_link">
    <img src="https://drive.google.com/thumbnail?id=1wjmoXu8vDhZXLRkSc_gd4nLa1-BJqCqa&sz=w1000"
         alt="DMD Process"
         style="max-width: 100%; height: auto;">
  </a>
</div>


Table of Contents

- [1. What Changed and Why](#what-changed-and-why)
- [2. The Math](#the-math)
- [3. Math and Code Side by Side](#math-and-code-side-by-side)
- [4. Training Algorithm](#training-algorithm)

---

## What Changed and Why

If you read the DMD post, you already know the core setup: a frozen teacher, a one-step student generator, and a trainable fake diffusion model that tracks the student's output distribution. The KL gradient is estimated as a score difference, and a stop-gradient proxy converts that into a differentiable loss. All of that stays.

What DMD also required was a regression anchor — a precomputed dataset of noise-image pairs $(z, y)$ where $y$ is produced by running the teacher's full ODE sampler from $z$. This anchor kept training stable and prevented mode dropping. But it came with real costs. Generating one pair for SDXL takes roughly 5 seconds. Covering the 12 million prompts in LAION-6.0 amounts to around 700 A100-GPU-days just for dataset construction, which is already more than 4x the total training compute of DMD2 itself. On top of the expense, the regression loss upper-bounds the student's quality: you are explicitly training the generator to reproduce the teacher's specific sampling trajectory, so the student can never do better than the teacher.

DMD2 removes the regression loss entirely. The question then becomes: what was the regression loss actually holding together, and how do you replace it? The answer the paper identifies is straightforward. The instability was not coming from the distribution-matching objective itself. It was coming from the fake diffusion model not tracking the generator's output distribution accurately enough. Because the generator changes every iteration, the fake score model is always playing catch-up. If it falls behind, the score difference signal becomes biased, and training drifts.

The fix is a Two Time-scale Update Rule (TTUR): update the fake diffusion model multiple times per generator step so it stays close to the current generator distribution. With that in place, the regression loss becomes unnecessary.

DMD2 then goes further in two directions. First, it adds a GAN objective on top of the existing distribution-matching loss. The discriminator is trained on real images, which means it does not inherit the teacher's score approximation errors. This lets the student actually surpass the teacher in quality. Second, it introduces a multi-step generator variant with a training procedure called backward simulation that eliminates the train-inference input mismatch that plagues previous multi-step distillation methods.

The three models are the same as in DMD, with one addition.

### The Teacher — A Frozen Base Diffusion Model

Same role as before. Provides the real score estimate $s_{\text{real}}$ via its mean-prediction network $\mu_{\text{real}}$. Frozen throughout. Its score approximation is good but not perfect, which is one of the motivations for the GAN term.

### The Student — A One-Step or Multi-Step Generator

In DMD this was strictly one-step. DMD2 extends it to support multi-step generation. The generator $G_\theta$ now takes both a noisy input $x_t$ and a timestep $t$ as arguments, so it can be chained across a fixed schedule of denoising steps during inference.

### The Critic — A Trainable Fake Diffusion Model with a Discriminator Head

This is where the architecture changes. The fake diffusion model $\mu_\phi^{\text{fake}}$ is still trained to denoise the generator's outputs, providing $s_{\text{fake}}$. But DMD2 attaches a classification head to the middle block of its UNet. This head is trained as the GAN discriminator, distinguishing real images from the generator's outputs. The encoder features upstream of the middle block are shared and updated by both the denoising loss and the GAN loss.

---

## The Math

### Notation

- Teacher diffusion model (frozen): $\mu_{\text{real}}(x_t, t)$
- Fake diffusion model (critic, trainable): $\mu_\phi^{\text{fake}}(x_t, t)$
- Discriminator (head on fake diffusion UNet, trainable): $D_\phi(x_t, t)$
- Generator (student, trainable): $G_\theta(x_t, t)$ — note it now conditions on $x_t$ and $t$, not just noise $z$

### The Distribution-Matching Objective — Unchanged

The core objective is still minimizing the forward KL divergence between the generator's output distribution and the real data distribution:

$$
D_{\text{KL}}\bigl(p_{\text{fake}} \,\|\, p_{\text{real}}\bigr).
$$

The gradient, derived in the DMD post, is

$$
\nabla_\theta D_{\text{KL}}
=
\mathbb{E}_{z}
\Bigl[
\bigl(
s_{\text{fake}}(x) - s_{\text{real}}(x)
\bigr)
\frac{\partial G_\theta(z)}{\partial \theta}
\Bigr].
$$

The score estimation via forward diffusion and mean-prediction networks is also the same. Apply forward diffusion:

$$
x_t = \alpha_t\, x + \sigma_t\, \epsilon, \qquad \epsilon \sim \mathcal{N}(0, I).
$$

The two scores are recovered from the respective diffusion models:

$$
s_{\text{real}}(x_t, t)
=
-\frac{x_t - \alpha_t\,\mu_{\text{real}}(x_t, t)}{\sigma_t^2},
\qquad
s_{\text{fake}}(x_t, t)
=
-\frac{x_t - \alpha_t\,\mu_\phi^{\text{fake}}(x_t, t)}{\sigma_t^2}.
$$

Subtracting:

$$
s_{\text{fake}}(x_t, t) - s_{\text{real}}(x_t, t)
=
\frac{\alpha_t}{\sigma_t^2}
\Bigl(
\mu_\phi^{\text{fake}}(x_t, t)
-
\mu_{\text{real}}(x_t, t)
\Bigr).
$$

This difference is the entire learning signal for the generator in the distribution-matching part of the objective.

### The Stop-Gradient Proxy Loss — Unchanged

Converting the gradient into a differentiable loss uses the same trick as DMD:

$$
\mathcal{L}_{\text{DM}}
=
\frac{1}{2}
\left\|
x_{\text{fake}}
-
\text{stopgrad}
\bigl(x_{\text{fake}} - w_t\,\alpha_t\,\Delta s\bigr)
\right\|_2^2,
$$

where $\Delta s = s_{\text{fake}} - s_{\text{real}}$ and the weight is

$$
w_t
=
\frac{\sigma_t^2}
{\|\mu_{\text{real}}(x_t, t) - x\|_1 + \varepsilon}.
$$

### What Is Gone — The Regression Loss

In DMD, the total generator loss was $\mathcal{L}_{\text{gen}} = \mathcal{L}_{\text{DM}} + \lambda_{\text{reg}}\,\mathcal{L}_{\text{reg}}$. The regression term required the precomputed paired dataset and the LPIPS loss against teacher ODE outputs. That entire term is removed. No paired dataset. No teacher sampling at training time.

### Why Removing It Broke Things — and What TTUR Fixes

When you naively drop the regression loss, training becomes unstable. The symptom is that global image statistics — pixel brightness being the most visible one — oscillate without converging. The cause is the fake diffusion model. It is optimized on a non-stationary distribution: the generator changes every step, and the fake model is always one step behind. The score estimate it provides is therefore biased, and that bias propagates directly into the generator's gradient.

TTUR, originally introduced in the context of FID estimation for GANs by Heusel et al., applies here directly. The idea is to run the fake diffusion model's optimizer at a higher frequency than the generator's. The paper finds that 5 fake model updates per 1 generator update is the right balance on ImageNet — fewer than 5 and the instability returns, more than 5 and you are just slowing down convergence for no gain. This keeps the fake score estimate close enough to the true score of the generator's current distribution that the gradient signal stays unbiased.

### The GAN Objective — New in DMD2

The distribution-matching loss alone, even when stabilized by TTUR, still cannot get the student past the teacher. The reason is that $s_{\text{real}}$ is estimated by the teacher diffusion model, and that estimate has its own approximation errors. Those errors flow directly into the generator gradient, putting a ceiling on quality.

The GAN loss provides a second training signal that does not go through the teacher at all. A discriminator $D_\phi$ is trained to distinguish real images from the generator's outputs. It is trained on actual real images, so it is not limited by the teacher's score approximation.

The standard non-saturating GAN objective is used:

$$
\mathcal{L}_{\text{GAN}}
=
\mathbb{E}_{x \sim p_{\text{real}},\, t}\bigl[\log D_\phi\bigl(F(x, t)\bigr)\bigr]
+
\mathbb{E}_{z \sim \mathcal{N}(0,I),\, t}\bigl[-\log D_\phi\bigl(F(G_\theta(z), t)\bigr)\bigr],
$$

where $F(\cdot, t)$ denotes applying the forward diffusion process at noise level $t$. The generator minimizes this objective; the discriminator maximizes it.

Note that the discriminator operates on *noised* images, not clean ones. This is the same smoothing trick used in DiffusionGAN and UFOGen — injecting noise before discrimination stabilizes the adversarial training dynamics significantly.

The discriminator itself is not a separate network. It is a small classification head attached to the middle block output of the fake diffusion UNet. The head is a stack of strided convolutions with group normalization and SiLU activations, downsampling to a $4 \times 4$ spatial resolution, followed by a single pooling convolution and a linear layer. The encoder features upstream are shared between the denoising task and the classification task, and are updated by both losses.

### Multi-Step Generation and Backward Simulation — New in DMD2

For large models like SDXL, a single forward pass does not have enough capacity to map noise to the full diversity of high-resolution images. DMD2 extends the generator to support $N$ denoising steps at inference.

The inference procedure is fixed: starting from $z_0 \sim \mathcal{N}(0, I)$, alternate between a denoising step and a forward diffusion step across a predetermined schedule $\{t_1, t_2, \ldots, t_N\}$:

$$
\hat{x}_{t_i} = G_\theta(x_{t_i},\, t_i), \qquad x_{t_{i+1}} = \alpha_{t_{i+1}}\,\hat{x}_{t_i} + \sigma_{t_{i+1}}\,\epsilon.
$$

The final output is $\hat{x}_{t_N}$. For the 4-step model the schedule is $\{999, 749, 499, 249\}$ when the teacher was trained with 1000 steps.

The training-inference mismatch problem in previous multi-step distillation methods is this: during training, the intermediate inputs $x_{t_i}$ are typically constructed by adding noise to *real* images. But during inference, except for the very first step, the inputs come from the *generator's own previous outputs*. The generator has never seen inputs like that during training, so quality degrades.

Backward simulation fixes this by constructing the training inputs the same way inference does. During training, run the current generator for several steps to produce a synthetic intermediate $\hat{x}_{t_i}$, then add noise to get $x_{t_{i+1}}$, and train on that. Because the generator is small (a few steps, not hundreds), this is tractable in a way it would not be for the teacher. The distribution-matching loss is particularly well-suited here: unlike a regression loss, it does not depend on the specific input to the generator, only on the output distribution. So the errors do not accumulate along the chain.

### The Total Generator Objective

Putting it all together, the generator is trained with:

$$
\mathcal{L}_{\text{gen}}
=
\mathcal{L}_{\text{DM}}
+
\lambda_{\text{GAN}}\,\mathcal{L}_{\text{GAN}}^{(\text{gen})},
$$

where $\mathcal{L}_{\text{GAN}}^{(\text{gen})}$ is the generator's side of the GAN objective (minimizing $-\log D_\phi$). No regression term.

---

<div align="center">
  <a href="https://drive.google.com/file/d/1e1u8NPYOG_PiUB1qbr0D-Z2Zcg35tWmw/view?usp=drive_link">
    <img src="https://drive.google.com/thumbnail?id=1e1u8NPYOG_PiUB1qbr0D-Z2Zcg35tWmw&sz=w1000"
         alt="DMD Process"
         style="max-width: 100%; height: auto;">
  </a>
</div>

<div align="center">
  <a href="https://drive.google.com/file/d/1wjmoXu8vDhZXLRkSc_gd4nLa1-BJqCqa/view?usp=drive_link">
    <img src="https://drive.google.com/thumbnail?id=1wjmoXu8vDhZXLRkSc_gd4nLa1-BJqCqa&sz=w1000"
         alt="DMD Process"
         style="max-width: 100%; height: auto;">
  </a>
</div>




## Math and Code Side by Side

The table below maps each component of DMD2 to PyTorch. The code follows the same conventions as the DMD post. Where a component is unchanged from DMD, the code is the same; new pieces are marked clearly.

| **Math** | **PyTorch** |
|---|---|
| **Generator sampling** $z \sim \mathcal{N}(0,I),\; x = G_\theta(z)$ | `z = torch.randn(B, C, device=device)` <br> `x_fake = generator(z)` |
| **Forward diffusion** $x_t = \alpha_t\, x + \sigma_t\, \epsilon$ | `noise = torch.randn_like(x_fake)` <br> `x_t = alpha * x_fake + sigma * noise` |
| **EDM-preconditioned denoised estimate** $\hat{x}_\psi = c_{\text{skip}}\,x_t + c_{\text{out}}\,f_\psi(c_{\text{in}}\,x_t, \sigma)$ | `eps = model(c_in * x_t, sigma)` <br> `x_pred = c_skip * x_t + c_out * eps` |
| **Score from mean prediction** $s = -(x_t - \alpha\,\hat{x}) / \sigma^2$ | `score = -(x_t - alpha * x_pred) / (sigma**2 + 1e-8)` |
| **Score difference** $\Delta s = s_{\text{fake}} - s_{\text{real}}$ | `s_real = compute_score(teacher, x_t, sigma)` <br> `s_fake = compute_score(fake_model, x_t, sigma)` <br> `delta_s = s_fake - s_real` |
| **Noise-dependent weighting** $w_t = \sigma_t^2 / (\|\mu_{\text{real}}(x_t,t) - x\|_1 + \varepsilon)$ | `den = torch.abs(x_pred_real - x_fake).mean(1, keepdim=True) + 1e-5` <br> `weight = (sigma**2) / den` <br> `weight = weight.clamp(0.1, 10.0)` |
| **DM loss (stop-gradient proxy)** $\mathcal{L}_{\text{DM}} = \tfrac{1}{2}\|x - \text{stopgrad}(x - w_t\,\alpha_t\,\Delta s)\|_2^2$ | `target = (x_fake - weight * alpha * delta_s).detach()` <br> `loss_dm = 0.5 * (x_fake - target).pow(2).mean()` |
| **Fake model denoising loss** $\mathcal{L}_{\text{fake}} = \mathbb{E}[w_t\,\|\hat{x}_{\text{fake}} - x_{\text{fake}}\|_2^2]$ | `x_hat_fake = fake_model(x_t, t)` <br> `loss_fake = (weight * (x_hat_fake - x_fake.detach()).pow(2)).mean()` |
| **TTUR: N fake updates per 1 generator update** | `for _ in range(N_FAKE_UPDATES):` <br> `    loss_fake = compute_fake_loss(...)` <br> `    loss_fake.backward()` <br> `    opt_fake.step(); opt_fake.zero_grad()` |
| **Discriminator forward pass** $D_\phi(x_t, t)$ — head on fake UNet middle block | `logits = fake_model.discriminate(x_t, t)` |
| **GAN loss — discriminator side** $\mathcal{L}_{\text{GAN}}^{(D)} = \mathbb{E}[\log D(x_t^{\text{real}})] + \mathbb{E}[-\log D(x_t^{\text{fake}})]$ | `loss_d_real = -torch.log(torch.sigmoid(logits_real) + 1e-8).mean()` <br> `loss_d_fake = -torch.log(1 - torch.sigmoid(logits_fake) + 1e-8).mean()` <br> `loss_disc = loss_d_real + loss_d_fake` |
| **GAN loss — generator side** $\mathcal{L}_{\text{GAN}}^{(G)} = \mathbb{E}[-\log D(x_t^{\text{fake}})]$ | `loss_gan_gen = -torch.log(torch.sigmoid(logits_fake) + 1e-8).mean()` |
| **Total generator loss** $\mathcal{L}_{\text{gen}} = \mathcal{L}_{\text{DM}} + \lambda_{\text{GAN}}\,\mathcal{L}_{\text{GAN}}^{(G)}$ | `loss_gen = loss_dm + LAMBDA_GAN * loss_gan_gen` <br> `loss_gen.backward()` <br> `opt_gen.step()` |
| **Multi-step inference schedule** $\hat{x}_{t_i} = G_\theta(x_{t_i}, t_i),\; x_{t_{i+1}} = \alpha_{t_{i+1}}\hat{x}_{t_i} + \sigma_{t_{i+1}}\epsilon$ | `for t_i, t_next in zip(schedule, schedule[1:]):` <br> `    x_hat = generator(x_t, t_i)` <br> `    x_t = alpha[t_next]*x_hat + sigma[t_next]*torch.randn_like(x_hat)` |
| **Backward simulation** — use generator outputs as training inputs, not noised real images | `with torch.no_grad():` <br> `    x_t_sim = simulate_backward(generator, z, schedule[:i])` <br> `x_hat = generator(x_t_sim, schedule[i])` <br> `# supervise x_hat with DM + GAN losses` |

---

## Training Algorithm

Each iteration now has three updates instead of two: the generator, the fake diffusion model (run multiple times), and the discriminator. The fake model and discriminator share encoder weights inside the same UNet, so their updates are interleaved within the same backward pass on the fake model side.

### Setup

**Given:** frozen teacher $\mu_{\text{real}}$, generator $G_\theta$, fake diffusion model $\mu_\phi^{\text{fake}}$ with attached discriminator head $D_\phi$, noise schedule $\sigma(t)$, TTUR ratio $N_{\text{fake}}$ (5 on ImageNet, 5–10 on text-to-image), GAN weight $\lambda_{\text{GAN}}$.

**Before training begins:** freeze the teacher, initialize $G_\theta$ and $\mu_\phi^{\text{fake}}$ from the teacher weights, create an EMA copy $\bar{G}_\theta$. No paired dataset is precomputed.

### Step A — Generator Update (1 step)

1. Sample noise and generate a fake image:
$$
z \sim \mathcal{N}(0, I), \qquad x_{\text{fake}} = G_\theta(z).
$$

2. Sample a noise level $t$ and apply forward diffusion:
$$
x_t = \alpha_t\, x_{\text{fake}} + \sigma_t\, \epsilon, \qquad \epsilon \sim \mathcal{N}(0, I).
$$

3. Compute both score estimates from the teacher and the fake model:
$$
s_{\text{real}}(x_t, t) = -\frac{x_t - \alpha_t\,\mu_{\text{real}}(x_t, t)}{\sigma_t^2},
\qquad
s_{\text{fake}}(x_t, t) = -\frac{x_t - \alpha_t\,\mu_\phi^{\text{fake}}(x_t, t)}{\sigma_t^2}.
$$

4. Form the weighted score difference and compute the distribution-matching loss:
$$
\Delta s = s_{\text{fake}} - s_{\text{real}},
\qquad
\mathcal{L}_{\text{DM}}
=
\frac{1}{2}
\left\|
x_{\text{fake}}
-
\text{stopgrad}\bigl(x_{\text{fake}} - w_t\,\alpha_t\,\Delta s\bigr)
\right\|_2^2.
$$

5. Run the discriminator on the noised fake image and compute the generator-side GAN loss. The generator wants the discriminator to classify its outputs as real:
$$
\mathcal{L}_{\text{GAN}}^{(G)} = -\log D_\phi\bigl(F(x_{\text{fake}}, t)\bigr).
$$

6. Update the generator with the combined objective:
$$
\mathcal{L}_{\text{gen}}
=
\mathcal{L}_{\text{DM}}
+
\lambda_{\text{GAN}}\,\mathcal{L}_{\text{GAN}}^{(G)}.
$$

### Step B — Fake Diffusion Model Update (N_fake steps)

This is the TTUR loop. Repeat $N_{\text{fake}}$ times per generator update.

1. Generate fake images with no gradient tracking:
$$
z \sim \mathcal{N}(0, I), \qquad x_{\text{fake}} = \text{stopgrad}\bigl(G_\theta(z)\bigr).
$$

2. Apply forward diffusion:
$$
x_t = \alpha_t\, x_{\text{fake}} + \sigma_t\, \epsilon.
$$

3. Predict the denoised output:
$$
\hat{x}_{\text{fake}} = \mu_\phi^{\text{fake}}(x_t, t).
$$

4. Compute and backpropagate the denoising loss:
$$
\mathcal{L}_{\text{fake}}
=
\mathbb{E}\!\left[
w_t\,\|\hat{x}_{\text{fake}} - x_{\text{fake}}\|_2^2
\right].
$$

5. Update fake diffusion parameters:
$$
\phi \leftarrow \phi - \eta_{\text{fake}}\,\nabla_\phi \mathcal{L}_{\text{fake}}.
$$

### Step C — Discriminator Update

The discriminator head shares the encoder with the fake diffusion model. Its update uses both real and fake images.

1. Sample a real image $x \sim p_{\text{real}}$ and a fake image $x_{\text{fake}} = \text{stopgrad}(G_\theta(z))$. Sample a noise level $t$ and apply forward diffusion to both:
$$
x_t^{\text{real}} = \alpha_t\, x + \sigma_t\, \epsilon, \qquad x_t^{\text{fake}} = \alpha_t\, x_{\text{fake}} + \sigma_t\, \epsilon.
$$

2. Run the discriminator on both:
$$
D_\phi(x_t^{\text{real}}), \qquad D_\phi(x_t^{\text{fake}}).
$$

3. Compute the non-saturating GAN loss and update the discriminator head (and shared encoder features):
$$
\mathcal{L}_{\text{GAN}}^{(D)}
=
-\mathbb{E}\bigl[\log D_\phi(x_t^{\text{real}})\bigr]
-
\mathbb{E}\bigl[\log\bigl(1 - D_\phi(x_t^{\text{fake}})\bigr)\bigr].
$$

### Multi-Step Variant — Backward Simulation During Training

When training the multi-step generator, the input to each denoising step is not a noised real image. It is the output of the generator's own previous step, noised back up. During training this is simulated explicitly:

1. Run the generator from $z$ through steps $t_1, \ldots, t_{i-1}$ with no gradient, producing a synthetic intermediate $\hat{x}_{t_{i-1}}$.

2. Add noise to get the input for step $i$:
$$
x_{t_i} = \alpha_{t_i}\,\hat{x}_{t_{i-1}} + \sigma_{t_i}\,\epsilon.
$$

3. Run the generator at step $i$ with gradient:
$$
\hat{x}_{t_i} = G_\theta(x_{t_i}, t_i).
$$

4. Supervise $\hat{x}_{t_i}$ with the same $\mathcal{L}_{\text{DM}} + \lambda_{\text{GAN}}\,\mathcal{L}_{\text{GAN}}^{(G)}$ losses as in the one-step case.

Because the distribution-matching loss does not depend on the specific input — only on the output distribution — the errors from earlier steps do not accumulate the way they would with a regression loss.


<div align="center">
  <a href="https://drive.google.com/file/d/10lsg-_7BSmazspKqUeWL9ZNykD34WYA1/view?usp=drive_link">
    <img src="https://drive.google.com/thumbnail?id=10lsg-_7BSmazspKqUeWL9ZNykD34WYA1&sz=w1000"
         alt="DMD Process"
         style="max-width: 100%; height: auto;">
  </a>
</div>


### Output

The EMA generator $\bar{G}_\theta$ is the final model. In the one-step variant it produces samples in a single forward pass. In the multi-step variant it produces samples across the fixed schedule $\{t_1, \ldots, t_N\}$. No teacher sampling is needed at inference.

---

To summarize what each piece is doing in DMD2: the generator learns where probability mass should move, guided by the score difference as before. The fake diffusion model learns what the generator's current distribution looks like, and TTUR keeps that estimate accurate enough that the score signal is unbiased. The GAN discriminator provides a second gradient that does not go through the teacher, which is what allows the student to surpass the teacher in quality. Backward simulation closes the loop for multi-step generation by making sure the generator sees the same kind of inputs during training that it will see during inference